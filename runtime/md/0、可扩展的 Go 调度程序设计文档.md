# 可扩展的 Go 调度程序设计文档

## 当前调度程序的问题

- 当前的 goroutine 调度器限制了用 Go 编写的并发程序的可扩展性，特别是高吞吐量服务器和并行计算程序。 Vtocc 服务器在 8 核机器上最大使用 70% CPU，而配置文件显示 14% 花费在 runtime.futex() 上。 通常，调度程序可能会禁止用户在性能至关重要的情况下使用惯用的细粒度并发。
- 当前实现有什么问题：
  1. 单一的全局互斥锁（Sched.Lock）和集中状态。 互斥锁保护所有与 goroutine 相关的操作（创建、完成、重新调度等）。
  2. Goroutine（G）切换（G.nextg）。 工作线程 (M) 经常在彼此之间切换可运行的 goroutine，这可能会导致延迟增加和额外开销。 每个 M 都必须能够执行任何可运行的 G，尤其是刚刚创建 G 的 M。
  3. 每M内存缓存（M.mcache）。 内存缓存和其他缓存（堆栈分配）与所有 M 相关联，而它们只需要与 M 运行的 Go 代码相关联（系统调用内部阻塞的 M 不需要 mcache）。 M 运行的 Go 代码与所有 M 之间的比率可以高达 1:100。 这会导致资源消耗过多（每个 MCache 最多可以吸收 2M）和数据局部性差。
  4. 积极的线程阻塞/解除阻塞。 在存在系统调用的情况下，工作线程经常被阻塞和解除阻塞。 这增加了很多开销。

## 设计

### 处理器 Processors

- 总体思路是将 P（处理器）的概念引入运行时，并在处理器之上实现工作窃取调度程序。
M 代表操作系统线程（就像现在一样）。 P 表示执行 Go 代码所需的资源。 当 M 执行 Go 代码时，它有一个关联的 P。当 M 空闲或在系统调用中时，它确实需要 P。
确实有 GOMAXPROCS P。 所有的 P 都被组织成一个数组，这是工作窃取的要求。 GOMAXPROCS 更改涉及停止/启动世界以调整 P 数组的大小。
来自 sched 的一些变量被分散并移至 P。来自 M 的一些变量移至 P（与 Go 代码的主动执行相关的变量）。

``` c
struct P
{
	Lock;
	G *gfree; // freelist, moved from sched
	G *ghead; // runnable, moved from sched
	G *gtail;
	MCache *mcache; // moved from M
	FixAlloc *stackalloc; // moved from M
	uint64 ncgocall;
	GCStats gcstats;
	// etc
	...
};

P *allp; // [GOMAXPROCS]
```

- 还有一个空闲P的无锁列表

```c
P *idlep; // lock-free list
```

- 当 M 愿意开始执行 Go 代码时，它必须从列表中弹出 P。 当 M 结束执行 Go 代码时，它将 P 推送到列表中。 所以，当 M 执行 Go 代码时，必须有一个关联的 P。这个机制替代了 sched.atomic (mcpu/mcpumax)。

### 调度 Scheduling

- 当一个新的 G 被创建或一个现有的 G 变为可运行时，它被推送到当前 P 的可运行 goroutines 列表中。 如果列表为空，P 会随机选择一个受害者（另一个 P）并尝试从中窃取一半可运行的 goroutine。

### 系统调用/M 停止和取消停止 **Syscalls/M Parking and Unparking**

- 当一个 M 创建一个新的 G 时，它必须确保有另一个 M 来执行 G（如果不是所有的 M 都已经忙了）。同样，当一个 M 进入 syscall 时，它必须确保有另一个 M 来执行 Go 代码。
  有两种选择，我们可以立即阻止和解除阻止 M，或者使用一些旋转。这是性能和消耗不必要的 CPU 周期之间的内在冲突。这个想法是使用旋转并燃烧 CPU 周期。但是，它不应影响使用 GOMAXPROCS=1 运行的程序（命令行实用程序、appengine 等）。
  自旋是两级的：（1）空闲的 M 与相关的 P 自旋寻找新的 G，（2）M 没有相关的 P 自旋等待可用的 P。最多有 GOMAXPROCS 旋转 M（（1）和（2））。当存在类型 (2) 的空闲 M 时，类型 (1) 的空闲 M 不会阻塞。
  当一个新的 G 被产生，或者 M 进入系统调用，或者 M 从空闲转换到忙碌时，它确保至少有 1 个旋转 M（或所有 P 都忙）。这确保没有可以运行的可运行 G；同时避免过多的M阻塞/解锁。
  自旋主要是被动的（屈服于操作系统，sched_yield()），但可能包括一点主动自旋（循环燃烧 CPU）（需要调查和调整）。

### 终止/死锁检测 Termination/Deadlock Detection

- 终止/死锁检测在分布式系统中更成问题。 一般的想法是仅在所有 P 空闲时（空闲 P 的全局原子计数器）才进行检查，这允许进行涉及 per-P 状态聚合的更昂贵的检查。
  还没有详细信息。

### 锁操作系统线程 LockOSThread

此功能不是性能关键。

1. 锁定的 G 变得不可运行（Gwaiting）。 M 立即将 P 返回到空闲列表，唤醒另一个 M 并阻塞。
2. 锁定 G 变为可运行（并到达 runq 的头部）。 当前 M 将自己的 P 和锁定的 G 移交给与锁定的 G 关联的 M，并解除对它的阻止。 当前 M 变为空闲。

### 空闲G **Idle G**

此功能不是性能关键。
有一个（或单个？）空闲 G 的全局队列。寻找工作的 M 在几次不成功的窃取尝试后检查队列。

## 实施计划 **Implementation Plan**

目标是将整个事情分成可以独立审查和提交的最小部分。

1. 引入P结构体（暂时为空）； 实现 allp/idlep 容器（idlep 对初学者来说是互斥保护的）； 将 P 与运行 Go 代码的 M 关联起来。 全局互斥量和原子状态仍然保留。
2. 将 G freelist 移动到 P。
3. 将 mcache 移动到 P。
4. 将 stackalloc 移动到 P。
5. 将 ncgocall/gcstats 移动到 P。
6. 去中心化运行队列，实现工作窃取。 消除 G 切换。 仍在全局互斥之下。
7. 去除全局互斥锁，实现分布式终止检测，LockOSThread。
8. 实施旋转而不是提示阻塞/解除阻塞。

该计划可能会行不通，有很多未探索的细节。

## 潜在的进一步改进 **Potential Further Improvements**

1. 尝试 LIFO 调度，这将提高局部性。 然而，它仍然必须提供一定程度的公平性并优雅地处理 yield goroutines。
2. 在goroutine第一次运行之前不要分配G和堆栈。 对于一个新创建的 goroutine，我们只需要 callerpc、fn、narg、nret 和 args，也就是大约 6 个单词。 这将允许创建大量运行到完成的 goroutine，内存开销显着降低。
3. G-to-P 更好的局部性。 尝试将未阻塞的 G 加入到它上次运行的 P 中。
4. P-to-M 更好的局部性。 尝试在上次运行的同一个 M 上执行 P。
5. M 创建的节流。 调度程序可以很容易地被迫每秒创建数千个 M，直到操作系统拒绝创建更多线程。 M 必须在 k*GOMAXPROCS 之前立即创建，之后可以通过计时器添加新 M。
